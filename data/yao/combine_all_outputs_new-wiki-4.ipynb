{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "which_wiki =4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# original_sentences\n",
    "with open(f'new_wiki_original_sentences_{which_wiki}.txt') as f:\n",
    "    original_sentences = f.readlines()\n",
    "    for i in range(len(original_sentences)):\n",
    "        original_sentences[i] = original_sentences[i].strip()\n",
    "\n",
    "# davinci-003\n",
    "with open(f'../../gpt_3/gpt-outputs/zero-shot-003-batch-{which_wiki}.json') as f:\n",
    "    gpt3 = json.load(f)\n",
    "\n",
    "# turbo\n",
    "with open(f'../../gpt_3/gpt-outputs/zero-shot-turbo-batch-{which_wiki}.json') as f:\n",
    "    turbo = json.load(f)\n",
    "\n",
    "# gpt-4\n",
    "with open(f\"../../gpt_3/gpt-outputs/zero-shot-gpt-4-batch-{which_wiki}.json\") as f:\n",
    "    gpt_4 = json.load(f)\n",
    "\n",
    "# alpaca-7b\n",
    "with open(f'alpaca-7B-new-wiki-{which_wiki}-output.txt') as f:\n",
    "    alpaca_7b = f.readlines()\n",
    "    for i in range(len(alpaca_7b)):\n",
    "        alpaca_7b[i] = alpaca_7b[i].strip()\n",
    "\n",
    "# vicuna-7b\n",
    "with open(f'vicuna-7B-new-wiki-{which_wiki}-output.txt') as f:\n",
    "    vicuna_7b = f.readlines()\n",
    "    for i in range(len(vicuna_7b)):\n",
    "        vicuna_7b[i] = vicuna_7b[i].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataframe with all outputs\n",
    "df = pd.DataFrame()\n",
    "df['Original Sentence'] = original_sentences\n",
    "df['Davinci-003'] = gpt3\n",
    "df['Turbo'] = turbo\n",
    "df['GPT-4'] = gpt_4\n",
    "df['Alpaca-7b'] = alpaca_7b\n",
    "df['Vicuna-7b'] = vicuna_7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'all-outputs-new-wiki-{which_wiki}-before-splitting.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/yaod_1/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "def remove_tokenization_artifacts(s, src):\n",
    "    stokens = s.split()\n",
    "    snew = s\n",
    "    for i, token in enumerate(stokens):\n",
    "        if i > 0 and i < len(stokens) - 1 and token in punctuation:\n",
    "            substrboth = stokens[i - 1] + token + stokens[i + 1]\n",
    "            substrleft = stokens[i - 1] + token\n",
    "            substright = token + stokens[i + 1]\n",
    "            if substrboth in src:\n",
    "                snew = snew.replace(stokens[i - 1] + \" \" + token + \" \" + stokens[i + 1], substrboth)\n",
    "            elif substrleft in src:\n",
    "                snew = snew.replace(stokens[i - 1] + \" \" + token, substrleft)\n",
    "            elif substright in src:\n",
    "                snew = snew.replace(token + \" \" + stokens[i + 1], substright)\n",
    "\n",
    "    snew = snew.replace(\"''\", '\"')\n",
    "    snew = snew.replace(\" .\", \".\")\n",
    "    snew_rest = \"\" if len(snew) == 1 else snew[1:]\n",
    "    if len(snew) > 0:\n",
    "        snew = snew[0].capitalize() + snew_rest\n",
    "    snew = snew.replace(\"-lrb-\", \"(\").replace(\"-rrb-\", \")\")\n",
    "    snew = snew.replace(\"-LRB-\", \"(\").replace(\"-RRB-\", \")\")\n",
    "    return snew\n",
    "\n",
    "def reformat_output(output, original):\n",
    "    output = remove_tokenization_artifacts(output, original)\n",
    "    output = \" || \".join(sent_tokenize(output))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat output with reformat_output function\n",
    "for index, row in df.iterrows():\n",
    "    df.loc[index, 'Davinci-003'] = reformat_output(row['Davinci-003'], row['Original Sentence'])\n",
    "    df.loc[index, 'Turbo'] = reformat_output(row['Turbo'], row['Original Sentence'])\n",
    "    df.loc[index, 'GPT-4'] = reformat_output(row['GPT-4'], row['Original Sentence'])\n",
    "    df.loc[index, 'Alpaca-7b'] = reformat_output(row['Alpaca-7b'], row['Original Sentence'])\n",
    "    df.loc[index, 'Vicuna-7b'] = reformat_output(row['Vicuna-7b'], row['Original Sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'all-outputs-new-wiki-{which_wiki}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('paraphrase')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "51d4b053945bb951b16f24566ff3322f1cf2f2e71937c6e2fa134fa8352168a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
