{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "random.seed(2)\n",
    "\n",
    "which_wiki =3\n",
    "\n",
    "# Read in the data\n",
    "df = pd.read_csv(f'humans-output-new-wiki-{which_wiki}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a id list for each row of df\n",
    "df['id'] = df.index\n",
    "\n",
    "cols = df.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random switch human 1 and human 2 and their writings\n",
    "for index, row in df.iterrows():\n",
    "    if random.random() > 0.5:\n",
    "        df.loc[index, 'Human 1'] = row['Human 2']\n",
    "        df.loc[index, 'Human 2'] = row['Human 1']\n",
    "        df.loc[index, 'Human 1 Writing'] = row['Human 2 Writing']\n",
    "        df.loc[index, 'Human 2 Writing'] = row['Human 1 Writing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(f'../gpt-3/gpt-outputs/zero-shot-batch-{which_wiki}.json') as f:\n",
    "    gpt3_zeroshot = json.load(f)\n",
    "\n",
    "# few-shot\n",
    "with open(f'../gpt-3/gpt-outputs/few-shot-batch-{which_wiki}.json') as f:\n",
    "    gpt3_fewshot = json.load(f)\n",
    "\n",
    "with open(f\"muss-output-new-wiki-{which_wiki}.json\") as f:\n",
    "    muss = json.load(f)\n",
    "\n",
    "# for new-wiki-3 t5-3B and 11B are txt\n",
    "with open(f'T5-3B-new-wiki-3-output.txt') as f:\n",
    "    t5_3B = f.readlines()\n",
    "    for i in range(len(t5_3B)):\n",
    "        t5_3B[i] = t5_3B[i].strip()\n",
    "\n",
    "with open(f'T5-11B-new-wiki-3-output.txt') as f:\n",
    "    t5_11B = f.readlines()\n",
    "    for i in range(len(t5_11B)):\n",
    "        t5_11B[i] = t5_11B[i].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['GPT-3-zero-shot'] = gpt3_zeroshot\n",
    "df['GPT-3-few-shot'] = gpt3_fewshot\n",
    "df['Muss'] = muss\n",
    "df['T5-3B'] = t5_3B\n",
    "df['T5-11B'] = t5_11B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove \\n in Human 1 Writing and Human 2 Writing\n",
    "df['Human 1 Writing'] = df['Human 1 Writing'].replace(\"\\n\", \"\", regex=True).apply(str)\n",
    "df['Human 2 Writing'] = df['Human 2 Writing'].replace(\"\\n\", \"\", regex=True).apply(str)\n",
    "\n",
    "# replace \"  \" with \" \"\n",
    "df['Human 1 Writing'] = df['Human 1 Writing'].replace(\"  \", \" \", regex=True).apply(str)\n",
    "df['Human 2 Writing'] = df['Human 2 Writing'].replace(\"  \", \" \", regex=True).apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'all-outputs-new-wiki-{which_wiki}-before-splitting.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/yaod_1/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "def remove_tokenization_artifacts(s, src):\n",
    "    stokens = s.split()\n",
    "    snew = s\n",
    "    for i, token in enumerate(stokens):\n",
    "        if i > 0 and i < len(stokens) - 1 and token in punctuation:\n",
    "            substrboth = stokens[i - 1] + token + stokens[i + 1]\n",
    "            substrleft = stokens[i - 1] + token\n",
    "            substright = token + stokens[i + 1]\n",
    "            if substrboth in src:\n",
    "                snew = snew.replace(stokens[i - 1] + \" \" + token + \" \" + stokens[i + 1], substrboth)\n",
    "            elif substrleft in src:\n",
    "                snew = snew.replace(stokens[i - 1] + \" \" + token, substrleft)\n",
    "            elif substright in src:\n",
    "                snew = snew.replace(token + \" \" + stokens[i + 1], substright)\n",
    "\n",
    "    snew = snew.replace(\"''\", '\"')\n",
    "    snew = snew.replace(\" .\", \".\")\n",
    "    snew_rest = \"\" if len(snew) == 1 else snew[1:]\n",
    "    if len(snew) > 0:\n",
    "        snew = snew[0].capitalize() + snew_rest\n",
    "    snew = snew.replace(\"-lrb-\", \"(\").replace(\"-rrb-\", \")\")\n",
    "    snew = snew.replace(\"-LRB-\", \"(\").replace(\"-RRB-\", \")\")\n",
    "    return snew\n",
    "\n",
    "def reformat_output(output, original):\n",
    "    output = remove_tokenization_artifacts(output, original)\n",
    "    output = \" || \".join(sent_tokenize(output))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat output with reformat_output function\n",
    "for index, row in df.iterrows():\n",
    "    df.loc[index, 'Human 1 Writing'] = reformat_output(row['Human 1 Writing'], row['Original Sentence'])\n",
    "    df.loc[index, 'Human 2 Writing'] = reformat_output(row['Human 2 Writing'], row['Original Sentence'])\n",
    "    df.loc[index, 'GPT-3-zero-shot'] = reformat_output(row['GPT-3-zero-shot'], row['Original Sentence'])\n",
    "    df.loc[index, 'GPT-3-few-shot'] = reformat_output(row['GPT-3-few-shot'], row['Original Sentence'])\n",
    "    df.loc[index, 'Muss'] = reformat_output(row['Muss'], row['Original Sentence'])\n",
    "    df.loc[index, 'T5-3B'] = reformat_output(row['T5-3B'], row['Original Sentence'])\n",
    "    df.loc[index, 'T5-11B'] = reformat_output(row['T5-11B'], row['Original Sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'all-outputs-new-wiki-{which_wiki}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('paraphrase')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "51d4b053945bb951b16f24566ff3322f1cf2f2e71937c6e2fa134fa8352168a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
