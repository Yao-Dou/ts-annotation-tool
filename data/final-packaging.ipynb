{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Injestion\n",
    "Create `simpeval_22_ajudicated.json` and `simpeval_ext_ajudicated.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:matplotlib data path: C:\\Users\\heine\\AppData\\Roaming\\Python\\Python39\\site-packages\\matplotlib\\mpl-data\n",
      "DEBUG:CONFIGDIR=C:\\Users\\heine\\.matplotlib\n",
      "DEBUG:interactive is False\n",
      "DEBUG:platform is win32\n",
      "DEBUG:CACHEDIR=C:\\Users\\heine\\.matplotlib\n",
      "DEBUG:Using fontManager instance from C:\\Users\\heine\\.matplotlib\\fontlist-v330.json\n",
      "INFO:Loading files: ['../data/inspection_rating_annotated/batch_1_ayush.json', '../data/inspection_rating_annotated/batch_1_rachel.json', '../data/inspection_rating_annotated/batch_1_vinayak.json', '../data/inspection_rating_annotated/batch_1_vishnesh.json', '../data/inspection_rating_annotated/batch_2_ayush.json', '../data/inspection_rating_annotated/batch_2_rachel.json', '../data/inspection_rating_annotated/batch_2_vinayak.json', '../data/inspection_rating_annotated/batch_2_vishnesh.json', '../data/inspection_rating_annotated/batch_3_ayush.json', '../data/inspection_rating_annotated/batch_3_rachel.json', '../data/inspection_rating_annotated/batch_3_vinayak.json', '../data/inspection_rating_annotated/batch_3_vishnesh.json', '../data/inspection_rating_annotated/batch_4_ayush.json', '../data/inspection_rating_annotated/batch_4_rachel.json', '../data/inspection_rating_annotated/batch_4_vinayak.json', '../data/inspection_rating_annotated/batch_4_vishnesh.json']\n",
      "\n",
      "INFO:Found users: {'ayush', 'vinayak', 'rachel', 'vishnesh'}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180: simpeval-22/GPT-3-few-shot\n",
      "180: simpeval-22/Muss\n",
      "180: simpeval-22/GPT-3-zero-shot\n",
      "180: simpeval-22/Human-1-written\n",
      "180: simpeval-22/T5-3B\n",
      "180: simpeval-22/Human-2-written\n",
      "180: simpeval-22/T5-11B\n",
      "\n",
      "120: simpeval-ext/Human-1-written\n",
      "120: simpeval-ext/GPT-3-zero-shot\n",
      "120: simpeval-ext/Human-2-written\n",
      "120: simpeval-ext/T5-3B\n",
      "120: simpeval-ext/T5-11B\n",
      "120: simpeval-ext/GPT-3-few-shot\n",
      "120: simpeval-ext/Muss\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../analysis')\n",
    "from utils.all import *\n",
    "from data_util import *\n",
    "log.basicConfig(level=log.INFO)\n",
    "data = load_data('../data/inspection_rating_annotated', preprocess=False, adjudicated=True)\n",
    "\n",
    "simpeval_22, simpeval_ext = [], []\n",
    "for sent in data:\n",
    "    if 'simpeval-22' in sent['system']:\n",
    "        simpeval_22 += [sent]\n",
    "    elif 'simpeval-ext' in sent['system']:\n",
    "        simpeval_ext += [sent]\n",
    "\n",
    "# Sanity check: Count the # annotations per system\n",
    "def number_annotations_per_system(data):\n",
    "    systems = set([s['system'] for s in data])\n",
    "    for system in systems:\n",
    "        print(f\"{len([s for s in data if s['system'] == system])}: {system}\")\n",
    "number_annotations_per_system(simpeval_22)\n",
    "print('')\n",
    "number_annotations_per_system(simpeval_ext)\n",
    "\n",
    "with open(f\"salsa/simpeval_22_ajudicated.json\", \"w\") as f:\n",
    "   json.dump(simpeval_22, f, indent=4)\n",
    "\n",
    "with open(f\"salsa/simpeval_ext_ajudicated.json\", \"w\") as f:\n",
    "   json.dump(simpeval_ext, f, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LENS-SALSA Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Loading files: ['../data/inspection_rating_annotated/batch_1_ayush.json', '../data/inspection_rating_annotated/batch_1_rachel.json', '../data/inspection_rating_annotated/batch_1_vinayak.json', '../data/inspection_rating_annotated/batch_1_vishnesh.json', '../data/inspection_rating_annotated/batch_2_ayush.json', '../data/inspection_rating_annotated/batch_2_rachel.json', '../data/inspection_rating_annotated/batch_2_vinayak.json', '../data/inspection_rating_annotated/batch_2_vishnesh.json', '../data/inspection_rating_annotated/batch_3_ayush.json', '../data/inspection_rating_annotated/batch_3_rachel.json', '../data/inspection_rating_annotated/batch_3_vinayak.json', '../data/inspection_rating_annotated/batch_3_vishnesh.json', '../data/inspection_rating_annotated/batch_4_ayush.json', '../data/inspection_rating_annotated/batch_4_rachel.json', '../data/inspection_rating_annotated/batch_4_vinayak.json', '../data/inspection_rating_annotated/batch_4_vishnesh.json']\n",
      "\n",
      "INFO:Found users: {'ayush', 'vinayak', 'rachel', 'vishnesh'}\n",
      "\n",
      "WARNING:rachel - Batch 1, HIT 1 (ID 420) has 3 deletion edits but 2 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:rachel - Batch 1, HIT 49 (ID 564) has 2 split edits but 1 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:rachel - Batch 1, HIT 52 (ID 573) has 1 split edits but 0 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:rachel - Batch 1, HIT 56 (ID 585) has 4 insertion edits but 3 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:rachel - Batch 1, HIT 97 (ID 708) has 7 substitution edits but 6 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:vinayak - Batch 2, HIT 307 (ID 920) has 2 substitution edits but 1 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:rachel - Batch 2, HIT 311 (ID 931) has 4 substitution edits but 3 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:rachel - Batch 2, HIT 321 (ID 961) has 9 substitution edits but 8 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:rachel - Batch 2, HIT 373 (ID 1117) has 1 split edits but 0 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:vinayak - Batch 3, HIT 488 (ID 1201) has 3 deletion edits but 2 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:vinayak - Batch 3, HIT 488 (ID 1201) has 5 structure edits but 4 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:ayush - Batch 3, HIT 573 (ID 1455) has 3 reorder edits but 2 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:rachel - Batch 3, HIT 579 (ID 1474) has 1 deletion edits but 0 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:rachel - Batch 3, HIT 587 (ID 1498) has 1 structure edits but 0 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:Annotation doesnt exist for edit\n",
      "WARNING:Annotation doesnt exist for edit\n",
      "WARNING:Annotation doesnt exist for edit\n",
      "WARNING:rachel - Batch 4, HIT 605 (ID 1552) has 5 substitution edits but 4 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:rachel - Batch 3, HIT 426 (ID 1635) has 1 deletion edits but 0 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:rachel - Batch 3, HIT 475 (ID 1782) has 2 structure edits but 1 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:rachel - Batch 3, HIT 479 (ID 1794) has 2 deletion edits but 1 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:rachel - Batch 4, HIT 673 (ID 2017) has 1 split edits but 0 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:rachel - Batch 4, HIT 675 (ID 2023) has 3 substitution edits but 2 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:rachel - Batch 4, HIT 679 (ID 2035) has 4 substitution edits but 3 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:Annotation doesnt exist for edit\n",
      "WARNING:rachel - Batch 4, HIT 686 (ID 2056) has 2 deletion edits but 0 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:rachel - Batch 4, HIT 686 (ID 2056) has 4 substitution edits but 0 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:rachel - Batch 4, HIT 686 (ID 2056) has 1 split edits but 0 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:rachel - Batch 4, HIT 686 (ID 2056) has 3 reorder edits but 0 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:rachel - Batch 4, HIT 686 (ID 2056) has 1 structure edits but 0 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:Annotation doesnt exist for edit\n",
      "WARNING:rachel - Batch 4, HIT 692 (ID 2074) has 5 deletion edits but 4 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:rachel - Batch 4, HIT 699 (ID 2095) has 1 insertion edits but 0 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:No deletion annotation found. Skipping...\n",
      "DEBUG:Couldn't process grammar for deletion: ['bad', '', 'no', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for deletion: ['good', 'a lot', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for deletion: ['good', 'minor', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process coreference error for deletion: ['bad', 'minor', '', 'no']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['', '', 'no', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process positive rating for substitution: ['', '', 'no', '', '']. Assuming 'somewhat'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'somewhat', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['very', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['minor', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for deletion: ['good', 'somewhat', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process coreference error for deletion: ['bad', 'minor', '', 'no']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['changes POS', '', 'positive', 'minor', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'somewhat', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['no', '', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process positive rating for substitution: ['no', '', '', '']. Assuming 'somewhat'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['no', '', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process positive rating for substitution: ['no', '', '', '']. Assuming 'somewhat'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'somewhat', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['changes clause', '', 'positive', 'minor', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['no', '', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process positive rating for substitution: ['no', '', '', '']. Assuming 'somewhat'...\n",
      "DEBUG:Couldn't process grammar for deletion: ['trivial', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process coreference error for deletion: ['bad', 'somewhat', '', 'no']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for deletion: ['good', 'minor', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for deletion: ['good', 'minor', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for deletion: ['good', 'minor', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for deletion: ['good', 'minor', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['no', '', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process positive rating for substitution: ['no', '', '', '']. Assuming 'somewhat'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'somewhat', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for deletion: ['trivial', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for deletion: ['trivial', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for deletion: ['good', 'somewhat', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['negative', '', 'a lot', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process positive rating for substitution: ['negative', '', 'a lot', '']. Assuming 'somewhat'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'somewhat', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process coreference error for deletion: ['bad', 'a lot', '', 'no']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process positive rating for substitution: ['changes tense', '', 'positive', '', 'no']. Assuming 'somewhat'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['no', '', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process positive rating for substitution: ['no', '', '', '']. Assuming 'somewhat'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['no', '', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process positive rating for substitution: ['no', '', '', '']. Assuming 'somewhat'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'somewhat', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['changes clause', '', 'no', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process positive rating for substitution: ['changes clause', '', 'no', '', '']. Assuming 'somewhat'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['changes grammatical number', '', 'positive', 'minor', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for deletion: ['good', 'minor', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for deletion: ['good', 'minor', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for deletion: ['trivial', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['no', '', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process positive rating for substitution: ['no', '', '', '']. Assuming 'somewhat'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['negative', '', 'minor', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process positive rating for substitution: ['negative', '', 'minor', '']. Assuming 'somewhat'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['no', '', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process positive rating for substitution: ['no', '', '', '']. Assuming 'somewhat'...\n",
      "DEBUG:Couldn't process grammar for deletion: ['good', 'minor', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['changes tense', 'minor', 'negative', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process positive rating for substitution: ['changes tense', 'minor', 'negative', '', '']. Assuming 'somewhat'...\n",
      "DEBUG:Couldn't process grammar for deletion: ['good', 'minor', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for deletion: ['good', 'minor', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['changes clause', '', 'positive', 'minor', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['changes POS', '', 'positive', 'minor', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['no', '', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process positive rating for substitution: ['no', '', '', '']. Assuming 'somewhat'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'somewhat', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'somewhat', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'somewhat', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n"
     ]
    }
   ],
   "source": [
    "data = load_data('../data/inspection_rating_annotated', preprocess=True, adjudicated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LENS scores\n",
    "with open('../lens/4-scores.json', 'r') as f:\n",
    "    scores = json.load(f)\n",
    "\n",
    "for sent in data:\n",
    "    for score in scores:\n",
    "        if sent['original'] == score['original'] and sent['simplified'] == score['simplified']:\n",
    "            sent['lens_score'] = score['lens']\n",
    "            sent['bleu'] = score['bleu']\n",
    "            sent['bertscore'] = score['bertscore']\n",
    "            sent['sari'] = score['sari']\n",
    "            sent['comet'] = score['comet']\n",
    "\n",
    "# Exclude corrupted sentences with no scores\n",
    "data = [s for s in data if 'bleu' in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lp,src,mt,ref,score,raw_score\n",
    "\n",
    "out_file = []\n",
    "for sent in data:\n",
    "    entry = {\n",
    "        # 'sentence_id': sent['sentence_id'],\n",
    "        'lp': 'en-en',\n",
    "        'src': sent['original'],\n",
    "        'mt': sent['simplified'],\n",
    "        'system': sent['system'],\n",
    "        'salsa_score': sent['score'],\n",
    "        'lens_score': sent['lens_score'],\n",
    "        \n",
    "        'bleu': sent['bleu'],\n",
    "        'bertscore': sent['bertscore'],\n",
    "        'sari': sent['sari'],\n",
    "        'comet': sent['comet'],\n",
    "        'simpeval_score_1': None,\n",
    "        'simpeval_score_2': None,\n",
    "        'simpeval_score_3': None,\n",
    "\n",
    "        # Subscores\n",
    "        'salsa_lexical_quality_score': sent['subscores']['quality_lexical'],\n",
    "        'salsa_syntax_quality_score': sent['subscores']['quality_syntax'],\n",
    "        'salsa_conceptual_quality_score': sent['subscores']['quality_content'],\n",
    "\n",
    "        'salsa_lexical_error_score': sent['subscores']['error_lexical'],\n",
    "        'salsa_syntax_error_score': sent['subscores']['error_syntax'],\n",
    "        'salsa_conceptual_error_score': sent['subscores']['error_content'],\n",
    "\n",
    "        'salsa_lexical_score': sent['subscores']['lexical'],\n",
    "        'salsa_syntax_score': sent['subscores']['syntax'],\n",
    "        'salsa_conceptual_score': sent['subscores']['content'],\n",
    "\n",
    "        'salsa_quality_score': sent['subscores']['quality'],\n",
    "        'salsa_error_score': sent['subscores']['error'],\n",
    "    }\n",
    "\n",
    "    if sent['simpeval_scores'] is not None:\n",
    "        entry.update({\n",
    "            'simpeval_score_1': sent['simpeval_scores'][0],\n",
    "            'simpeval_score_2': sent['simpeval_scores'][1],\n",
    "            'simpeval_score_3': sent['simpeval_scores'][2],\n",
    "        })\n",
    "\n",
    "    # Generate word-level QE\n",
    "    for family_constraint in [None] + list(Family):\n",
    "        for sentence_type in ['original', 'simplified']:\n",
    "            tags = get_annotations_per_token([sent], sentence_type, remove_none=False, tagging=True)\n",
    "            tags_by_type = get_tag_values(tags, family_constraint)\n",
    "\n",
    "            for tag_type, tag_value in tags_by_type.items():\n",
    "                tag_value = write_tagged_sentence(sent[sentence_type], tags, tag_value)\n",
    "                \n",
    "                tag_value = tag_value.replace('<ok>', '')\\\n",
    "                    .replace('</ok>', '').replace('<noedit>', '')\\\n",
    "                    .replace('</noedit>', '')\n",
    "            \n",
    "                fam_name = f'_{Family.CONTENT.value.lower()}' if family_constraint is not None else ''\n",
    "                entry.update({\n",
    "                    f'{tag_type}_{sentence_type}{fam_name}': tag_value\n",
    "                })\n",
    "    \n",
    "    # Generate edit type tagging\n",
    "    for sentence_type in ['original', 'simplified']:\n",
    "        tags = get_annotations_per_token([sent], sentence_type, collapse_composite=True, remove_reorder=True, remove_none=False)\n",
    "        tags_by_type = get_edit_values(tags)\n",
    "\n",
    "        for tag_type, tag_value in tags_by_type.items():\n",
    "            tag_value = write_tagged_sentence(sent[sentence_type], tags, tag_value)\n",
    "\n",
    "            tag_value = tag_value.replace('<ok>', '').replace('</ok>', '')\n",
    "        \n",
    "            entry.update({\n",
    "                f'{tag_type}_{sentence_type}': tag_value\n",
    "            })\n",
    "\n",
    "    # Generate traditional alignment setup\n",
    "    entry.update({\n",
    "        'alignment': get_word_alignment_string(sent),\n",
    "        'alignment-no-phrases': get_word_alignment_string(sent, collapse_phrase_alignment=True),\n",
    "        'alignment-error-labels-input': ' '.join(get_tag_values(get_annotations_per_token([sent], 'original', \\\n",
    "            remove_none=False, tagging=True))['word_qe_error_types']),\n",
    "        'alignment-error-labels-output': ' '.join(get_tag_values(get_annotations_per_token([sent], 'simplified', \\\n",
    "            remove_none=False, tagging=True))['word_qe_error_types']),\n",
    "    })\n",
    "\n",
    "    out_file += [entry]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\textit{13057} & \\textit{628} & \\textit{449} & \\textit{1901}\n"
     ]
    }
   ],
   "source": [
    "# Get number of edit types\n",
    "edits = [i for j in [s['processed_annotations'] for s in data] for i in j]\n",
    "num_error = sum([e['error_type'] is not None for e in edits])\n",
    "num_complex = sum([e['error_type'] == Error.COMPLEX_WORDING for e in edits])\n",
    "num_bad_del = sum([e['error_type'] == Error.BAD_DELETION for e in edits])\n",
    "num_quality = len(edits) - num_error\n",
    "print(\"\\\\textit{\" + str(num_quality) + \"} & \\\\textit{\" + str(num_bad_del) + \"} & \\\\textit{\" + str(num_complex) + \"} & \\\\textit{\" + str(num_error) + \"}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'edit_type': 'deletion',\n",
       " 'id': 0,\n",
       " 'information_impact': <Information.LESS: 'Generalization'>,\n",
       " 'type': <Quality.TRIVIAL: 'Trivial'>,\n",
       " 'subtype': None,\n",
       " 'family': <Family.LEXICAL: 'Lexical'>,\n",
       " 'grammar_error': False,\n",
       " 'error_type': None,\n",
       " 'rating': None,\n",
       " 'size': 0.008310249307479225,\n",
       " 'token_size': 1,\n",
       " 'reorder_level': None,\n",
       " 'original_span': [(92, 95)],\n",
       " 'simplified_span': None,\n",
       " 'score': 0.0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\textit{13057} & \\textit{628} & \\textit{449} & \\textit{1901} & -- & \\\\\n",
      "\\textit{55K} & \\textit{4.3K} & \\textit{1.8K} & \\textit{12K} & \\textit{82K} & \\\\\n"
     ]
    }
   ],
   "source": [
    "# Get number of tokens\n",
    "edits = [i for j in [s['processed_annotations'] for s in data] for i in j]\n",
    "num_error = [e['token_size'] for e in edits if e['error_type'] is not None]\n",
    "num_complex = [e['token_size'] for e in edits if e['error_type'] == Error.COMPLEX_WORDING]\n",
    "num_bad_del = [e['token_size'] for e in edits if e['error_type'] == Error.BAD_DELETION]\n",
    "num_quality = [e['token_size'] for e in edits if e['error_type'] is None] # e['type'] == Quality.QUALITY\n",
    "num_same_tok = sum([len(s['original'].split(' ') + s['simplified'].split(' ')) for s in data]) - sum([e['token_size'] for e in edits])\n",
    "\n",
    "print(\"\\\\textit{\" + str(len(num_quality)) + \"} & \\\\textit{\" + str(len(num_bad_del)) + \"} & \\\\textit{\" + str(len(num_complex)) + \"} & \\\\textit{\" + str(len(num_error)) + \"} & -- & \\\\\\\\\")\n",
    "print(\"\\\\textit{\" + str(int(sum(num_quality)/1000)) + \"K} & \\\\textit{\" + str(round(sum(num_bad_del)/1000, 1)) + \"K} & \\\\textit{\" + str(round(sum(num_complex)/1000, 1)) + \"K} & \\\\textit{\" + str(round(sum(num_error)/1000)) + \"K} & \\\\textit{\" + str(round(num_same_tok/1000)) + \"K} & \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67914"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split by src sentence, so we don't have val data that's trained on outputs\n",
    "src_sents = list(set(s['src'] for s in out_file))\n",
    "split = int(len(src_sents) * 0.7)\n",
    "src_sents_train, src_sents_val = src_sents[:split], src_sents[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_csv('salsa/lens-salsa-training/train.csv', [s for s in out_file if s['src'] in src_sents_train])\n",
    "write_csv('salsa/lens-salsa-training/valid.csv', [s for s in out_file if s['src'] in src_sents_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write with & without collapsed alignment\n",
    "write_tsv_align('salsa/lens-salsa-training/train-align.tsv', [s for s in out_file if s['src'] in src_sents_train])\n",
    "write_tsv_align('salsa/lens-salsa-training/valid-align.tsv', [s for s in out_file if s['src'] in src_sents_val])\n",
    "\n",
    "write_tsv_align('salsa/lens-salsa-training/train-align-collapsed.tsv', [s for s in out_file if s['src'] in src_sents_train], collapse_phrase_alignment=True)\n",
    "write_tsv_align('salsa/lens-salsa-training/valid-align-collapsed.tsv', [s for s in out_file if s['src'] in src_sents_val], collapse_phrase_alignment=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0-0 0-1 0-2 1-0 1-1 1-2 2-0 2-1 2-2 3-0 3-1 3-2 4-0 4-1 4-2 5-3 6-4 7-5 8-5 9-6 10-7 11-8 12-11 14-12 15-13 16-14 17-15 18-16 19-17 19-18 19-19 19-20 19-21 20-17 20-18 20-19 20-20 20-21 21-17 21-18 21-19 21-20 21-21 22-22 23-23 24-24 24-25 25-33 26-34 26-35 27-36 28-37 29-38 30-39 31-40 32-41 33-42 34-43 35-44 36-45 37-46 38-47 39-48 40-49 41-50 42-51 45-26 45-27 45-28 45-29 46-26 46-27 46-28 46-29 47-26 47-27 47-28 47-29 48-26 48-27 48-28 48-29 49-26 49-27 49-28 49-29 50-26 50-27 50-28 50-29 51-26 51-27 51-28 51-29 52-26 52-27 52-28 52-29 53-26 53-27 53-28 53-29'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collapse_phrase_alignment = False\n",
    "\n",
    "orig_tags = get_annotations_per_token([sent], 'original', collapse_composite=True, remove_reorder=True, \\\n",
    "    remove_none=False, get_alignment=True)\n",
    "simp_tags = get_annotations_per_token([sent], 'simplified', collapse_composite=True, remove_reorder=True, \\\n",
    "    remove_none=False, get_alignment=True)\n",
    "\n",
    "word_alignment = get_word_alignment(orig_tags, simp_tags, sent)\n",
    "alignment = align_edits(word_alignment, orig_tags, simp_tags, sent, collapse_phrase_alignment)\n",
    "\n",
    "orig_tags = get_annotations_per_token([sent], 'original', collapse_composite=True, remove_reorder=True, \\\n",
    "    remove_none=False)\n",
    "simp_tags = get_annotations_per_token([sent], 'simplified', collapse_composite=True, remove_reorder=True, \\\n",
    "    remove_none=False)\n",
    "\n",
    "orig_ids = {k: i for i, k in enumerate(orig_tags.keys()) if orig_tags[k] is not set()}\n",
    "simp_ids = {k: i for i, k in enumerate(simp_tags.keys()) if simp_tags[k] is not set()}\n",
    "\n",
    "' '.join([f\"{orig_ids[x[0]]}-{simp_ids[x[1]]}\" for x in alignment])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
