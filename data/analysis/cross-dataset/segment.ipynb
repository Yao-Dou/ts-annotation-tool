{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Injestion\n",
    "Parses different dataset formats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### College Admissions\n",
    "[github](https://zenodo.org/record/7114359#.Y6ILC3bMKXI) / [paper](https://jantrienes.com/assets/papers/tsar2022.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_dist(s1, s2):\n",
    "    if len(s1) > len(s2):\n",
    "        s1, s2 = s2, s1\n",
    "\n",
    "    distances = range(len(s1) + 1)\n",
    "    for i2, c2 in enumerate(s2):\n",
    "        distances_ = [i2+1]\n",
    "        for i1, c1 in enumerate(s1):\n",
    "            if c1 == c2:\n",
    "                distances_.append(distances[i1])\n",
    "            else:\n",
    "                distances_.append(1 + min((distances[i1], distances[i1 + 1], distances_[-1])))\n",
    "        distances = distances_\n",
    "    return distances[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take from the test set, some sentences not clean enough to include\n",
    "test_set_ids = [4, 30, 38, 52, 65, 71, 77, 91, 97, 100, 106, 107, 109, 119, 121, 149, 166, 172, 174, 184, 192, 199, 200, 215, 221, 227, 250, 253, 258, 259, 278, 286, 309]\n",
    "exclude = [97]\n",
    "test_set_ids = [x for x in test_set_ids if x not in exclude]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "admissions = []\n",
    "for id_ in test_set_ids:\n",
    "    alignment = f'admissions/alignments/alignments_{id_}.txt'\n",
    "    original = f'admissions/original/original_{id_}.txt'\n",
    "    simplified = f'admissions/simplified/simplified_{id_}.txt'\n",
    "\n",
    "    with open(alignment, 'r', encoding='utf-8') as f:\n",
    "        alignment = [x.replace('(', '').replace(')', '').replace(' ', '').split(',') for x in f.read().split('\\n')]\n",
    "        alignment = [[int(x[0]), [int(y) for y in x[1:]]] for x in alignment if len(x) != 1 and x[1] != 'N']\n",
    "\n",
    "    with open(original, 'r', encoding='utf-8') as f:\n",
    "        original = f.read().split('\\n')\n",
    "\n",
    "    with open(simplified, 'r', encoding='utf-8') as f:\n",
    "        simplified = f.read().split('\\n')\n",
    "\n",
    "    # Identify simplified sentences with multiple source alignments (i.e. sent fusion)\n",
    "    align_map = {}\n",
    "    for align in alignment:\n",
    "        for simp_align in align[1]:\n",
    "            if simp_align not in align_map:\n",
    "                align_map[simp_align] = []\n",
    "        align_map[simp_align] += [align[0]]\n",
    "    n_to_1 = [i for j in [v for k, v in align_map.items() if len(v) > 1] for i in j]\n",
    "\n",
    "    for align in alignment:\n",
    "        orig_sent = original[align[0]]\n",
    "        simp_sent = ''.join([simplified[i] for i in align[1]])\n",
    "\n",
    "        # Add space after periods, except the last period\n",
    "        simp_sent = simp_sent[:-1].replace('.', '. ') + simp_sent[-1]\n",
    "        \n",
    "        ed = edit_dist(orig_sent, simp_sent)\n",
    "\n",
    "        if (\n",
    "            align[0] not in n_to_1 and  # Our interface does not support N:1 simplification. Throw these out.\n",
    "            orig_sent != simp_sent and  # Exlude identical sentences\n",
    "            ed > 50 and                 # Exclude sentences with minimal change\n",
    "            len(orig_sent) > 50 and     # Exclude short sentences\n",
    "            len(simp_sent) > 50         # Exclude sentences which delete almost all original information. Typically this is because they are contained elsewhere\n",
    "        ):\n",
    "            admissions += [{\n",
    "                'original': orig_sent,\n",
    "                'simplified': simp_sent\n",
    "            }]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medical Transcriptions\n",
    "[github](https://github.com/babylonhealth/laymaker) / [paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8861686/pdf/3576988.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "path = 'medical-transcriptions/test.csv'\n",
    "\n",
    "with open(path, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    keys = next(reader)\n",
    "    contents = [row for row in reader]\n",
    "    loaded = []\n",
    "    for sent in contents:\n",
    "        loaded += [{k: v for k, v in zip(keys, sent)}]\n",
    "\n",
    "medical = []\n",
    "for sent in loaded:\n",
    "    orig_sent = sent['ORIGINAL']\n",
    "    simp_sent = sent['REFERENCE']\n",
    "\n",
    "    ed = edit_dist(orig_sent, simp_sent)\n",
    "\n",
    "    if (\n",
    "        orig_sent != simp_sent and  # Exlude identical sentences\n",
    "        ed > 45 and                 # Exclude sentences with minimal change\n",
    "        len(orig_sent) > 40 and     # Exclude short sentences\n",
    "        len(simp_sent) > 40         # Exclude sentences which delete almost all original information. Typically this is because they are contained elsewhere\n",
    "    ):\n",
    "        medical += [{\n",
    "            'original': orig_sent,\n",
    "            'simplified': simp_sent\n",
    "        }]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medical Abstracts\n",
    "[github](https://github.com/AshOlogn/Paragraph-level-Simplification-of-Medical-Texts) / [paper](https://aclanthology.org/2021.naacl-main.395/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('devaraj-medical/data-1024.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "devaraj = []\n",
    "for sent in data:\n",
    "    orig_sent = sent['abstract']\n",
    "    simp_sent = sent['pls']\n",
    "\n",
    "    devaraj += [{\n",
    "        'original': orig_sent,\n",
    "        'simplified': simp_sent\n",
    "    }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are paragraphs. This will *not* work without a sentence alignment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clinical Notes\n",
    "*\"Sharing of the dataset is currently underway.\"*\n",
    "\n",
    "[github](https://github.com/jantrienes/simple-patho) / [paper](https://jantrienes.com/assets/papers/tsar2022.pdf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newsela\n",
    "[paper](https://aclanthology.org/Q15-1021/)\n",
    "\n",
    "Data is not publically available, must request access. We'll be taking the lest/most complicated versions of the simplification. Should cite the paper that does this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# TODO: CAREFUL!\n",
    "path = 'newsela/crowdsourced/train.tsv'\n",
    "\n",
    "newsela = []\n",
    "with open(path, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    keys = ['alignment', 'simp_info', 'orig_info', 'simplified', 'original']\n",
    "    contents = [row for row in reader]\n",
    "    loaded = []\n",
    "    for sent in contents:\n",
    "        loaded += [{k: v for k, v in zip(keys, sent)}]\n",
    "\n",
    "for sent in loaded:\n",
    "    sent['article'], orig_info = sent['orig_info'].split('.')\n",
    "    sent['language'], sent['orig_readability'], sent['orig_pg_idx'], sent['orig_sent_idx'] = orig_info.split('-')\n",
    "\n",
    "    sent['article'], simp_info = sent['simp_info'].split('.')\n",
    "    sent['language'], sent['simp_readability'], sent['simp_pg_idx'], sent['simp_sent_idx'] = simp_info.split('-')\n",
    "\n",
    "    sent['orig_readability'] = int(sent['orig_readability'])\n",
    "    sent['simp_readability'] = int(sent['simp_readability'])\n",
    "\n",
    "loaded = [x for x in loaded if 'original' in x.keys()]\n",
    "\n",
    "for sent in loaded:\n",
    "    if (\n",
    "        sent['alignment'] == 'aligned' and\n",
    "        sent['orig_readability'] == 0\n",
    "    ):\n",
    "        # Find the original sentence\n",
    "        curr = sent\n",
    "        level = 0\n",
    "        while level != 4:\n",
    "            cands = [x for x in loaded if x['orig_info'] == curr['simp_info']]\n",
    "            if len(cands) != 1 or cands[0]['alignment'] != 'aligned' or len(cands[0]['simplified']) < 10:\n",
    "                break\n",
    "            curr = cands[0]\n",
    "            level += 1\n",
    "        if level == 0:\n",
    "            continue\n",
    "\n",
    "        newsela += [{\n",
    "            'original': sent['original'],\n",
    "            'simplified': curr['simplified'],\n",
    "            # 'orig_readability': sent['orig_readability'],\n",
    "            # 'simp_readability': curr['simp_readability']\n",
    "        }]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'multi-domain/admissions': admissions,\n",
    "    'multi-domain/medical-transcriptions': medical,\n",
    "    # 'multi-domain/devaraj': devaraj,\n",
    "    'multi-domain/newsela': newsela\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def prepare_sent(original, simplified, system, id_):\n",
    "    current_dict = {}\n",
    "    current_dict[\"id\"] = id_\n",
    "    current_dict[\"original\"] = original.strip()\n",
    "    current_dict[\"original_spans\"] = []\n",
    "\n",
    "    simplified = simplified.strip()\n",
    "    simplified = re.sub(r\" 's\", \"'s\", simplified)\n",
    "    simplified = simplified.replace('. ', '. || ')\n",
    "\n",
    "    current_dict[\"simplified\"] = simplified\n",
    "    current_dict[\"simplified_spans\"] = []\n",
    "\n",
    "    if \"||\" in simplified:\n",
    "        #  find index of all || in simplified\n",
    "        indices = [m.start() for m in re.finditer('\\|\\|', simplified)]\n",
    "        for i, indice in enumerate(indices):\n",
    "            current_dict[\"simplified_spans\"].append([2, indice, indice+2, i])\n",
    "    \n",
    "    current_dict[\"system\"] = f\"system\"\n",
    "    return current_dict\n",
    "\n",
    "system_list = data.keys()\n",
    "annotators = ['yao', 'david']\n",
    "batch_size = 20\n",
    "\n",
    "id_ = 0\n",
    "for i, annotator in enumerate(annotators):\n",
    "    batch = []\n",
    "    for system, dataset in data.items():\n",
    "        segment = batch_size // len(data.keys())\n",
    "        for sent in dataset[i*segment:(i+1)*segment]:\n",
    "            curr = prepare_sent(sent['original'], sent['simplified'], system, id_)\n",
    "            batch += [curr]\n",
    "    with open(f\"batches/{annotator}.json\", \"w\") as f:\n",
    "        json.dump(batch, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
