{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-class Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:matplotlib data path: C:\\Users\\heine\\AppData\\Roaming\\Python\\Python39\\site-packages\\matplotlib\\mpl-data\n",
      "DEBUG:CONFIGDIR=C:\\Users\\heine\\.matplotlib\n",
      "DEBUG:interactive is False\n",
      "DEBUG:platform is win32\n",
      "DEBUG:CACHEDIR=C:\\Users\\heine\\.matplotlib\n",
      "DEBUG:Using fontManager instance from C:\\Users\\heine\\.matplotlib\\fontlist-v330.json\n",
      "INFO:Loading files: ['../data/inspection_rating_annotated/batch_1_ayush.json', '../data/inspection_rating_annotated/batch_1_rachel.json', '../data/inspection_rating_annotated/batch_1_vinayak.json', '../data/inspection_rating_annotated/batch_1_vishnesh.json', '../data/inspection_rating_annotated/batch_2_ayush.json', '../data/inspection_rating_annotated/batch_2_rachel.json', '../data/inspection_rating_annotated/batch_2_vinayak.json', '../data/inspection_rating_annotated/batch_2_vishnesh.json', '../data/inspection_rating_annotated/batch_3_ayush.json', '../data/inspection_rating_annotated/batch_3_rachel.json', '../data/inspection_rating_annotated/batch_3_vishnesh.json', '../data/inspection_rating_annotated/batch_4_ayush.json', '../data/inspection_rating_annotated/batch_4_rachel.json', '../data/inspection_rating_annotated/batch_4_vishnesh.json']\n",
      "\n",
      "INFO:Found users: {'rachel', 'vishnesh', 'ayush', 'vinayak'}\n",
      "\n",
      "WARNING:rachel - Batch 1, HIT 1 (ID 420) has 3 deletion edits but 2 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:rachel - Batch 1, HIT 49 (ID 564) has 2 split edits but 1 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:rachel - Batch 1, HIT 52 (ID 573) has 1 split edits but 0 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:rachel - Batch 1, HIT 56 (ID 585) has 4 insertion edits but 3 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:rachel - Batch 1, HIT 97 (ID 708) has 7 substitution edits but 6 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:vinayak - Batch 2, HIT 307 (ID 920) has 2 substitution edits but 1 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:rachel - Batch 2, HIT 311 (ID 931) has 4 substitution edits but 3 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:rachel - Batch 2, HIT 321 (ID 961) has 9 substitution edits but 8 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:rachel - Batch 2, HIT 373 (ID 1117) has 1 split edits but 0 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:ayush - Batch 3, HIT 573 (ID 1363) has 3 reorder edits but 2 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:rachel - Batch 3, HIT 579 (ID 1382) has 1 deletion edits but 0 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:rachel - Batch 3, HIT 587 (ID 1406) has 1 structure edits but 0 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:Annotation doesnt exist for edit\n",
      "WARNING:Annotation doesnt exist for edit\n",
      "WARNING:Annotation doesnt exist for edit\n",
      "WARNING:rachel - Batch 4, HIT 605 (ID 1460) has 5 substitution edits but 4 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:rachel - Batch 3, HIT 426 (ID 1518) has 1 deletion edits but 0 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:rachel - Batch 3, HIT 475 (ID 1616) has 2 structure edits but 1 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:rachel - Batch 3, HIT 479 (ID 1624) has 2 deletion edits but 1 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:rachel - Batch 4, HIT 673 (ID 1795) has 1 split edits but 0 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:rachel - Batch 4, HIT 675 (ID 1799) has 3 substitution edits but 2 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:rachel - Batch 4, HIT 679 (ID 1807) has 4 substitution edits but 3 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:Annotation doesnt exist for edit\n",
      "WARNING:rachel - Batch 4, HIT 686 (ID 1821) has 2 deletion edits but 0 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:rachel - Batch 4, HIT 686 (ID 1821) has 4 substitution edits but 0 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:rachel - Batch 4, HIT 686 (ID 1821) has 1 split edits but 0 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:rachel - Batch 4, HIT 686 (ID 1821) has 3 reorder edits but 0 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:rachel - Batch 4, HIT 686 (ID 1821) has 1 structure edits but 0 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:rachel - Batch 4, HIT 692 (ID 1833) has 5 deletion edits but 4 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:rachel - Batch 4, HIT 699 (ID 1847) has 1 insertion edits but 0 annotations. Likely a missing annotation. Skipping edit type...\n",
      "WARNING:No deletion annotation found. Skipping...\n",
      "DEBUG:Couldn't process grammar for deletion: ['bad', '', 'no', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for deletion: ['good', 'a lot', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for deletion: ['good', 'minor', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process coreference error for deletion: ['bad', 'minor', '', 'no']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['', '', 'no', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process positive rating for substitution: ['', '', 'no', '', '']. Assuming 'somewhat'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'somewhat', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['very', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['minor', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for deletion: ['good', 'somewhat', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process coreference error for deletion: ['bad', 'minor', '', 'no']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['changes POS', '', 'positive', 'minor', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'somewhat', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['no', '', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process positive rating for substitution: ['no', '', '', '']. Assuming 'somewhat'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['no', '', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process positive rating for substitution: ['no', '', '', '']. Assuming 'somewhat'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'somewhat', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['changes clause', '', 'positive', 'minor', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['no', '', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process positive rating for substitution: ['no', '', '', '']. Assuming 'somewhat'...\n",
      "DEBUG:Couldn't process grammar for deletion: ['trivial', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process coreference error for deletion: ['bad', 'somewhat', '', 'no']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for deletion: ['good', 'minor', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for deletion: ['good', 'minor', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for deletion: ['good', 'minor', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for deletion: ['good', 'minor', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['no', '', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process positive rating for substitution: ['no', '', '', '']. Assuming 'somewhat'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'somewhat', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for deletion: ['trivial', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for deletion: ['trivial', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for deletion: ['good', 'somewhat', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'somewhat', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process positive rating for substitution: ['changes tense', '', 'positive', '', 'no']. Assuming 'somewhat'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['no', '', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process positive rating for substitution: ['no', '', '', '']. Assuming 'somewhat'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['no', '', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process positive rating for substitution: ['no', '', '', '']. Assuming 'somewhat'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'somewhat', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['changes clause', '', 'no', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process positive rating for substitution: ['changes clause', '', 'no', '', '']. Assuming 'somewhat'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['changes grammatical number', '', 'positive', 'minor', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for deletion: ['good', 'minor', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for deletion: ['good', 'minor', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for deletion: ['trivial', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['no', '', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process positive rating for substitution: ['no', '', '', '']. Assuming 'somewhat'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['negative', '', 'minor', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process positive rating for substitution: ['negative', '', 'minor', '']. Assuming 'somewhat'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['no', '', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process positive rating for substitution: ['no', '', '', '']. Assuming 'somewhat'...\n",
      "DEBUG:Couldn't process grammar for deletion: ['good', 'minor', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['changes tense', 'minor', 'negative', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process positive rating for substitution: ['changes tense', 'minor', 'negative', '', '']. Assuming 'somewhat'...\n",
      "DEBUG:Couldn't process grammar for deletion: ['good', 'minor', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for deletion: ['good', 'minor', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['changes clause', '', 'positive', 'minor', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['changes POS', '', 'positive', 'minor', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['no', '', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process positive rating for substitution: ['no', '', '', '']. Assuming 'somewhat'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'somewhat', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'somewhat', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'somewhat', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n",
      "DEBUG:Couldn't process grammar for substitution: ['positive', 'minor', '', '']. Assuming 'no'...\n"
     ]
    }
   ],
   "source": [
    "from utils.all import *\n",
    "from scipy.stats import kendalltau, pearsonr, spearmanr\n",
    "\n",
    "# data = load_data('../annotated', batch_num=[5, 6, 7, 8, 9], preprocess=True)\n",
    "data = load_data('../data/inspection_rating_annotated', preprocess=True, adjudicated=True)\n",
    "\n",
    "for sent in data:\n",
    "    for annotation in sent['processed_annotations']:\n",
    "        annotation['score'] = calculate_annotation_score(annotation, default_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add pre-computed scores\n",
    "with open('../lens/4-scores.json', 'r') as f:\n",
    "    scores = json.load(f)\n",
    "scores = [s for s in scores if 'bleu' in s.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally compute LENS-SALSA and add\n",
    "# rsync dheineman3@sky1.cc.gatech.edu:/nethome/dheineman3/nlprx/lens-salsa/5-scores-lens-salsa.json /mnt/c/Users/heine/Documents/research/ts-annotation-tool/lens/5-scores-lens-salsa.json\n",
    "# with open('../lens/5-scores-lens-salsa.json', 'r') as f:\n",
    "#     scores = json.load(f)\n",
    "# scores = [s for s in scores if 'bleu' in s.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1842"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating sensitivity with scores for 1304 sentences\n"
     ]
    }
   ],
   "source": [
    "# Add our scores\n",
    "conditions = [\n",
    "    'quality_content', \n",
    "    'quality_syntax', \n",
    "    'quality_lexical', \n",
    "    'error_content', \n",
    "    'error_syntax', \n",
    "    'error_lexical', \n",
    "    'quality', \n",
    "    'error', \n",
    "    'all'\n",
    "]\n",
    "\n",
    "for s in scores:\n",
    "    s['system'] = s['system'] \\\n",
    "        .replace('simpeval-22', 'new-wiki-1') \\\n",
    "        .replace('simpeval-ext', 'new-wiki-1')\n",
    "\n",
    "# Create a scores list with references and simplifications\n",
    "our_scores = []\n",
    "for orig in set([x['original'] for x in data]):\n",
    "    sents = [sent for sent in data if sent['original'] == orig]\n",
    "\n",
    "    human = [sent for sent in sents if 'Human' in sent['system']]\n",
    "    systems = [sent for sent in sents if 'Human' not in sent['system']] # Or you can try just GPT-3-Few\n",
    "\n",
    "    if len(systems) == 0:\n",
    "        continue\n",
    "\n",
    "    for system in systems:\n",
    "        # print(system['system'])\n",
    "        # print(score['system'])\n",
    "        aligned = [sent for sent in scores if \n",
    "            sent['original'] == orig and\n",
    "            sent['system'] in system['system'] \n",
    "        ]\n",
    "\n",
    "        if len(aligned) == 0:\n",
    "            continue\n",
    "            \n",
    "        n_score = aligned[0]\n",
    "        \n",
    "        n_score['our_score'] = system['score']\n",
    "        for condition in conditions:\n",
    "            n_score[f'our_score_{condition}'] = calculate_sentence_score(system, get_params(condition))\n",
    "            # if 'error' in condition:\n",
    "            #     n_score[f'our_score_{condition}'] = -n_score[f'our_score_{condition}']\n",
    "\n",
    "        our_scores += [n_score]\n",
    "\n",
    "print(f\"Calculating sensitivity with scores for {len(our_scores)} sentences\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = our_scores.copy()\n",
    "scores = [s for s in scores if 'comet' in s.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_name_mapping = {\n",
    "    'quality_lexical': 'Lexical',\n",
    "    'quality_syntax': 'Syntax',\n",
    "    'quality_content': 'Conceptual',\n",
    "    'error_lexical': 'Lexical',\n",
    "    'error_syntax': 'Syntax',\n",
    "    'error_content': 'Conceptual',\n",
    "    'error': 'All Error',\n",
    "    'quality': 'All Quality',\n",
    "    'all': 'All Edits'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\multirow{3}{*}{\\rotatebox[origin=c]{90}{Quality}} & Lexical & -0.210 & \\underline{0.064} & -0.055 & -0.014 & \\textbf{0.260} \\tabularnewline\n",
      "& Syntax & -0.200 & \\underline{0.075} & -0.073 & -0.115 & \\textbf{0.199} \\tabularnewline\n",
      "& Conceptual & -0.174 & \\underline{-0.076} & -0.351 & -0.346 & \\textbf{0.180} \\tabularnewline\n",
      "\\midrule\n",
      "\\multirow{3}{*}{\\rotatebox[origin=c]{90}{Error}} & Lexical & -0.262 & -0.131 & -0.085 & \\textbf{-0.002} & -0.018 \\tabularnewline\n",
      "& Syntax & -0.128 & \\underline{-0.052} & -0.100 & \\textbf{-0.015} & -0.055 \\tabularnewline\n",
      "& Conceptual & -0.092 & \\underline{-0.058} & -0.324 & -0.316 & \\textbf{0.147} \\tabularnewline\n",
      "\\midrule\n",
      "\\multirow{3}{*}{\\rotatebox[origin=c]{90}{All}} & All Error & -0.256 & \\underline{-0.138} & -0.347 & -0.262 & \\textbf{0.089} \\tabularnewline\n",
      "& All Quality & -0.255 & \\underline{0.097} & -0.076 & -0.054 & \\textbf{0.307} \\tabularnewline\n",
      "& All Edits & -0.309 & \\underline{0.009} & -0.260 & -0.235 & \\textbf{0.281} \\tabularnewline\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate Kendall Tau correlation for each statistic\n",
    "from scipy.stats import kendalltau, pearsonr, spearmanr\n",
    "import heapq\n",
    "metrics = ['bleu', 'sari', 'bertscore', 'comet', 'lens']\n",
    "all_results = []\n",
    "prec = 3\n",
    "\n",
    "# Calculate metric corrleation\n",
    "for metric in metrics:\n",
    "    sys_results = []\n",
    "    for condition in condition_name_mapping.keys():\n",
    "        p = pearsonr(\n",
    "            [s[f'our_score_{condition}'] for s in scores if s[metric] is not None], \n",
    "            [s[metric] for s in scores if s[metric] is not None]\n",
    "        )\n",
    "        sp = spearmanr(\n",
    "            [s[f'our_score_{condition}'] for s in scores if s[metric] is not None], \n",
    "            [s[metric] for s in scores if s[metric] is not None]\n",
    "        )\n",
    "        results = (round(sp[0], prec), round(p[0], prec))\n",
    "        sys_results += [results]\n",
    "    all_results += [sys_results]\n",
    "\n",
    "# Render LaTeX table\n",
    "delimiters = [\n",
    "    '\\multirow{3}{*}{\\\\rotatebox[origin=c]{90}{Quality}}',\n",
    "    '\\multirow{3}{*}{\\\\rotatebox[origin=c]{90}{Error}}',\n",
    "    '\\multirow{3}{*}{\\\\rotatebox[origin=c]{90}{All}}'\n",
    "]\n",
    "\n",
    "out = ''\n",
    "for i, condition in enumerate(condition_name_mapping.keys()):\n",
    "    line = ''\n",
    "\n",
    "    if i % 3 == 0:\n",
    "        if i != 0:\n",
    "            line += '\\\\midrule\\n'\n",
    "        line += f'{delimiters[int(i/3)]} '\n",
    "        \n",
    "    line += f'& {condition_name_mapping[condition]} & '\n",
    "    a_max_p, b_max_p = heapq.nlargest(2, [x[i][0] for x in all_results])\n",
    "    a_max_sp, b_max_sp = heapq.nlargest(2, [x[i][1] for x in all_results])\n",
    "    for j, metric in enumerate(metrics):\n",
    "        p = all_results[j][i][0]\n",
    "        sp = all_results[j][i][1]\n",
    "\n",
    "        if str(p) == 'nan':\n",
    "            p = '---'\n",
    "        if str(sp) == 'nan':\n",
    "            sp = '---'\n",
    "\n",
    "        if p == a_max_p:\n",
    "            p = f'\\\\textbf{{{round(p, prec):.3f}}}'\n",
    "        elif sp == a_max_sp:\n",
    "            sp = f'\\\\textbf{{{round(sp, prec):.3f}}}'\n",
    "        elif p == b_max_p:\n",
    "            p = f'\\\\underline{{{round(p, prec):.3f}}}'\n",
    "        elif sp == b_max_sp:\n",
    "            sp = f'\\\\underline{{{round(sp, prec):.3f}}}'\n",
    "        else:\n",
    "            p = f'{round(p, prec):.3f}'\n",
    "\n",
    "        # line += f'{p} & {sp} & '\n",
    "        line += f'{p} & '\n",
    "    out += line[:-2] + '\\\\tabularnewline\\n'\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (<ipython-input-9-6aaf1f276005>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-9-6aaf1f276005>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    break\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation between Qualtiy and Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.all import *\n",
    "\n",
    "data = load_data('../annotated', batch_num=[5, 6, 7, 8, 9, 10, 11], preprocess=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the each score for all unique sentences\n",
    "# For each sentence, we have 5 generations and 3 annotations per generation\n",
    "# Of the 5 generations, 2 are ASSET human references, 1 is TurkCorpus human reference\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [5, 4]\n",
    "\n",
    "conditions = [\n",
    "    'quality_content', \n",
    "    'quality_syntax', \n",
    "    'quality_lexical', \n",
    "    'error_content', \n",
    "    'error_syntax', \n",
    "    'error_lexical', \n",
    "    'quality', \n",
    "    'error', \n",
    "    'all'\n",
    "]\n",
    "\n",
    "scores = []\n",
    "for sent in data:\n",
    "    our_score = sent['score']\n",
    "\n",
    "    score = {\n",
    "        'original': sent['original'],\n",
    "        'simplified': sent['simplified'],\n",
    "        'errors': list(set([ann['error_type'] for ann in sent['processed_annotations'] if ann['error_type'] != None])),\n",
    "        'our_score': our_score\n",
    "    }\n",
    "\n",
    "    # The next goal is to calculate a bunch of variations on our score considering only\n",
    "    # certain dimensions of the metric\n",
    "    for condition in conditions:\n",
    "        score[f'our_score_{condition}'] = calculate_sentence_score(sent, get_params(condition))\n",
    "\n",
    "    scores += [score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts = [(score['our_score_quality'], -score['our_score_error']) for score in scores]\n",
    "print(f'{(100*sum([x[1] == 0 for x in pts]) / len(pts)):.2f}% of sentences are error-less')\n",
    "pts = [x for x in pts if x[1] > 0]\n",
    "\n",
    "plt.scatter([p[0] for p in pts], [p[1] for p in pts], c =\"red\", alpha=0.2)\n",
    "plt.xlabel('Our score (Only Quality)')\n",
    "plt.ylabel('Our score (Only Error, negated)')\n",
    "plt.title(f'Error vs. Quality for Error Sentences ({len(pts)} sentences)')\n",
    "out_filename = f'../paper/plot/error-vs-quality.pdf'\n",
    "plt.savefig(out_filename, format=\"pdf\", bbox_inches='tight', pad_inches=0.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts = [(score['our_score_quality_lexical'], score['our_score_quality_content']) for score in scores]\n",
    "plt.scatter([p[0] for p in pts], [p[1] for p in pts], c =\"red\", alpha=0.2)\n",
    "plt.xlabel('Our score (Only Lexical)')\n",
    "plt.ylabel('Our score (Only Content)')\n",
    "plt.title(f'Lexical vs. Content Quality ({len(pts)} sentences)')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components of Simplification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph simplicity/adequacy/fluency vs simpeval score\n",
    "# graph content/syntax/grammar vs simpeval score\n",
    "# graph content vs syntax, syntax vs grammar, grammar vs content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data('../annotated', batch_num=[5, 6], preprocess=True) # [1, 2, 3, 4]\n",
    "systems = set([x['system'] for x in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    'quality_content', \n",
    "    'quality_syntax', \n",
    "    'quality_lexical', \n",
    "    'error_content', \n",
    "    'error_syntax', \n",
    "    'error_lexical', \n",
    "    'quality', \n",
    "    'error', \n",
    "    'all'\n",
    "]\n",
    "\n",
    "total_sent = 0\n",
    "fig, axs = plt.subplots(2, 3, tight_layout=True, figsize=(10, 6))\n",
    "for system in systems:    \n",
    "    fluency, adequacy, simplicity = [], [], []\n",
    "    our_score = []\n",
    "    for sent in [s for s in data if s['system'] == system]:\n",
    "        if sent['simpeval_scores'] != None and sent['da_scores'] != None:\n",
    "            # edit distance\n",
    "            # scores += [(\n",
    "            #     avg(sent['simpeval_scores'], prec=10), \n",
    "            #     edit_dist(sent['original'], sent['simplified'])\n",
    "            # )]\n",
    "            score = {}\n",
    "            for condition in conditions:\n",
    "                score[f'our_score_{condition}'] = calculate_sentence_score(sent, get_params(condition))\n",
    "            \n",
    "            fluency += [(\n",
    "                avg([x['fluency'] for x in sent['da_scores']]),\n",
    "                avg(sent['simpeval_scores'], prec=10)\n",
    "            )]\n",
    "            adequacy += [(\n",
    "                avg([x['adequacy'] for x in sent['da_scores']]),\n",
    "                avg(sent['simpeval_scores'], prec=10)\n",
    "            )]\n",
    "            simplicity += [(\n",
    "                avg([x['simplicity'] for x in sent['da_scores']]),\n",
    "                avg(sent['simpeval_scores'], prec=10)\n",
    "            )]\n",
    "\n",
    "            out_score = []\n",
    "            for condition in ['quality_content', 'quality_syntax', 'quality_lexical']:\n",
    "                out_score += [(\n",
    "                    score[f'our_score_{condition}'],\n",
    "                    avg(sent['simpeval_scores'], prec=10)\n",
    "                )]\n",
    "            our_score += [out_score]\n",
    "                \n",
    "\n",
    "        total_sent += 1\n",
    "    # scores = [(sent['score'], edit_dist(sent['original'], sent['simplified'])) for sent in data if sent['system'] == system]\n",
    "\n",
    "    axs[0, 0].scatter([p[0] for p in fluency], [p[1] for p in fluency], c=color_mapping[system], alpha=0.5, label=system_name_mapping[system])\n",
    "    axs[0, 1].scatter([p[0] for p in adequacy], [p[1] for p in adequacy], c=color_mapping[system], alpha=0.5, label=system_name_mapping[system])\n",
    "    axs[0, 2].scatter([p[0] for p in simplicity], [p[1] for p in simplicity], c=color_mapping[system], alpha=0.5, label=system_name_mapping[system])\n",
    "\n",
    "    for i in range(3):\n",
    "        axs[1, i].scatter([p[i][0] for p in our_score], [p[i][1] for p in our_score], c=color_mapping[system], alpha=0.5, label=system_name_mapping[system])\n",
    "\n",
    "axs[0, 0].set_ylabel('SimpEval Score')\n",
    "axs[1, 0].set_ylabel('SimpEval Score')\n",
    "\n",
    "axs[0, 0].set_xlabel('Fluency')\n",
    "axs[0, 1].set_xlabel('Adequacy')\n",
    "axs[0, 2].set_xlabel('Simplicity')\n",
    "\n",
    "axs[1, 0].set_xlabel('Lexical')\n",
    "axs[1, 1].set_xlabel('Syntax')\n",
    "axs[1, 2].set_xlabel('Content')\n",
    "\n",
    "plt.suptitle(f'Components of Simplification ({total_sent} sentences)')\n",
    "# font_size = 8\n",
    "# legend_loc = (0, 0)\n",
    "# plt.legend(loc='lower center', bbox_to_anchor=legend_loc,\n",
    "#             fancybox=True, ncol=2, borderaxespad=1.,fontsize=font_size,\n",
    "#             facecolor='white',edgecolor='black',framealpha=1,frameon=False,\n",
    "#             columnspacing=1,handlelength=1,handleheight=1,handletextpad=0.4,\n",
    "#             borderpad=0.2)\n",
    "plt.legend(loc='lower right', bbox_to_anchor=(2, 0), frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    'quality_content', \n",
    "    'quality_syntax', \n",
    "    'quality_lexical', \n",
    "    'error_content', \n",
    "    'error_syntax', \n",
    "    'error_lexical', \n",
    "    'quality', \n",
    "    'error', \n",
    "    'all'\n",
    "]\n",
    "\n",
    "plots = [[\n",
    "    ('fluency', 'adequacy'),\n",
    "    ('adequacy', 'simplicity'),\n",
    "    ('simplicity', 'fluency')\n",
    "], [\n",
    "    ('quality_lexical', 'quality_syntax'),\n",
    "    ('quality_syntax', 'quality_content'),\n",
    "    ('quality_content', 'quality_lexical')\n",
    "]]\n",
    "\n",
    "total_sent = 0\n",
    "fig, axs = plt.subplots(2, 3, tight_layout=True, figsize=(10, 6))\n",
    "for system in systems:    \n",
    "    pts = []\n",
    "    for sent in [s for s in data if s['system'] == system]:\n",
    "        if sent['simpeval_scores'] != None and sent['da_scores'] != None:\n",
    "            # edit distance\n",
    "            # scores += [(\n",
    "            #     avg(sent['simpeval_scores'], prec=10), \n",
    "            #     edit_dist(sent['original'], sent['simplified'])\n",
    "            # )]\n",
    "            score = {}\n",
    "            for condition in conditions:\n",
    "                score[f'our_score_{condition}'] = calculate_sentence_score(sent, get_params(condition))\n",
    "            \n",
    "            pts += [[[(\n",
    "                avg([x['fluency'] for x in sent['da_scores']]),\n",
    "                avg([x['adequacy'] for x in sent['da_scores']]),\n",
    "            ), (\n",
    "                avg([x['adequacy'] for x in sent['da_scores']]),\n",
    "                avg([x['simplicity'] for x in sent['da_scores']]),\n",
    "            ), (\n",
    "                avg([x['simplicity'] for x in sent['da_scores']]),\n",
    "                avg([x['fluency'] for x in sent['da_scores']]),\n",
    "            )], [(\n",
    "                score[f'our_score_quality_lexical'],\n",
    "                score[f'our_score_quality_syntax'],\n",
    "            ), (\n",
    "                score[f'our_score_quality_syntax'],\n",
    "                score[f'our_score_quality_content'],\n",
    "            ), (\n",
    "                score[f'our_score_quality_content'],\n",
    "                score[f'our_score_quality_lexical']\n",
    "            )]]]\n",
    "        total_sent += 1\n",
    "\n",
    "    for i in range(2):\n",
    "        for j in range(3):\n",
    "            axs[i, j].scatter([p[i][j][0] for p in pts], [p[i][j][1] for p in pts], c=color_mapping[system], alpha=0.5, label=system_name_mapping[system])\n",
    "\n",
    "axs[0, 0].set_xlabel('Fluency')\n",
    "axs[0, 0].set_ylabel('Adequacy')\n",
    "\n",
    "axs[0, 1].set_xlabel('Adequacy')\n",
    "axs[0, 1].set_ylabel('Simplicity')\n",
    "\n",
    "axs[0, 2].set_xlabel('Simplicity')\n",
    "axs[0, 2].set_ylabel('Fluency')\n",
    "\n",
    "axs[1, 0].set_xlabel('Lexical')\n",
    "axs[1, 0].set_ylabel('Syntax')\n",
    "\n",
    "axs[1, 1].set_xlabel('Syntax')\n",
    "axs[1, 1].set_ylabel('Content')\n",
    "\n",
    "axs[1, 2].set_xlabel('Content')\n",
    "axs[1, 2].set_ylabel('Lexical')\n",
    "\n",
    "plt.suptitle(f'Correlation Between Components of Simplification ({total_sent} sentences)')\n",
    "# font_size = 8\n",
    "# legend_loc = (0, 0)\n",
    "# plt.legend(loc='lower center', bbox_to_anchor=legend_loc,\n",
    "#             fancybox=True, ncol=2, borderaxespad=1.,fontsize=font_size,\n",
    "#             facecolor='white',edgecolor='black',framealpha=1,frameon=False,\n",
    "#             columnspacing=1,handlelength=1,handleheight=1,handletextpad=0.4,\n",
    "#             borderpad=0.2)\n",
    "plt.legend(loc='lower right', bbox_to_anchor=(2, 0), frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the percentile of a score among some systems\n",
    "from scipy.stats import percentileofscore\n",
    "system = 'GPT-3-few'\n",
    "raw_scores = [9, 6.7, 1.46]\n",
    "i = 0\n",
    "for dim in ['content', 'syntax', 'lexical']:\n",
    "    scores = []\n",
    "    for sent in [x for x in data if system in x['system']]:\n",
    "        scores += [\n",
    "            calculate_sentence_score(sent, get_params(f'quality_{dim}')) +\n",
    "            calculate_sentence_score(sent, get_params(f'error_{dim}'))\n",
    "        ]\n",
    "\n",
    "    percentile_of_3 = percentileofscore(scores, raw_scores[i])\n",
    "\n",
    "    i += 1\n",
    "    print(percentile_of_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_score = 0.617\n",
    "scores = []\n",
    "system = 'Human'\n",
    "for sent in [x for x in data if system in x['system']]:\n",
    "    simpeval_scores = sent['simpeval_scores']\n",
    "    if simpeval_scores != None:\n",
    "        scores += [avg(simpeval_scores)]\n",
    "percentileofscore(scores, raw_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_score = 96.4\n",
    "scores = []\n",
    "system = 'GPT-3-few'\n",
    "for sent in [x for x in data if system in x['system']]:\n",
    "    if sent['da_scores'] != None:\n",
    "        da_scores = [avg(x.values()) for x in sent['da_scores']]\n",
    "        scores += [avg(da_scores)]\n",
    "percentileofscore(scores, raw_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit Reversing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problems with reversing edits:\n",
    "    # Overlapping edits (of different types)\n",
    "    # Deletions have no mapping to the original sentence\n",
    "    # Re-orders have no mapping to their original location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My approach\n",
    "    # use generate_token_dict(s) to get a list of token spans for the input/output\n",
    "    # iterate through edits (in some predetermined priority order) to list the operations we perform on each word\n",
    "    # perform those operations to collapse the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphs import *\n",
    "\n",
    "id_ = 40\n",
    "sents = [sent for sent in data if sent['id'] == id_]\n",
    "draw_agreement(sents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
